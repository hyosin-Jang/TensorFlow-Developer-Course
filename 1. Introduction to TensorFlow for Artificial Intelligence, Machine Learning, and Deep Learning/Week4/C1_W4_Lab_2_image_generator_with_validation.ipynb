{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "C1_W4_Lab_2_image_generator_with_validation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ImageDataGenerator with a validation set"
      ],
      "metadata": {
        "id": "dHMrxCqsiA33"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the datasets `horse-or-human.zip` and `validation-horse-or-human.zip`"
      ],
      "metadata": {
        "id": "QRajlO-GiOhD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KrsFN-2h-5N",
        "outputId": "8a042b6e-97d7-4198-e26f-22cdffef1781"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1onaG42NZft3wCE1WH0GDEbUhu75fedP5\n",
            "To: /content/horse-or-human.zip\n",
            "100% 150M/150M [00:01<00:00, 116MB/s] \n"
          ]
        }
      ],
      "source": [
        "!gdown --id 1onaG42NZft3wCE1WH0GDEbUhu75fedP5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1LYeusSEIiZQpwN-mthh5nKdA75VsKG1U"
      ],
      "metadata": {
        "id": "CLEk9E9LiL3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then unzip both archives."
      ],
      "metadata": {
        "id": "C-Gk6SNOiXWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Unzip training set\n",
        "local_zip = './horse-or-human.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('./horse-or-human')\n",
        "\n",
        "# Unzip validation set\n",
        "local_zip = './validation-horse-or-human.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('./validation-horse-or-human')\n",
        "\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "I8aQsdp7iM1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train dir과 validation dir 디렉토리 정의하기"
      ],
      "metadata": {
        "id": "MpDluOi3iaVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Directory with training horse pictures\n",
        "train_horse_dir = os.path.join('./horse-or-human/horses')\n",
        "\n",
        "# Directory with training human pictures\n",
        "train_human_dir = os.path.join('./horse-or-human/humans')\n",
        "\n",
        "# Directory with validation horse pictures\n",
        "validation_horse_dir = os.path.join('./validation-horse-or-human/horses')\n",
        "\n",
        "# Directory with validation human pictures\n",
        "validation_human_dir = os.path.join('./validation-horse-or-human/humans')"
      ],
      "metadata": {
        "id": "BHjdQKOIiaBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_horse_names = os.listdir(train_horse_dir)\n",
        "print(f'TRAIN SET HORSES: {train_horse_names[:10]}')\n",
        "\n",
        "train_human_names = os.listdir(train_human_dir)\n",
        "print(f'TRAIN SET HUMANS: {train_human_names[:10]}')\n",
        "\n",
        "validation_horse_names = os.listdir(validation_horse_dir)\n",
        "print(f'VAL SET HORSES: {validation_horse_names[:10]}')\n",
        "\n",
        "validation_human_names = os.listdir(validation_human_dir)\n",
        "print(f'VAL SET HUMANS: {validation_human_names[:10]}')"
      ],
      "metadata": {
        "id": "KZ9Dx5eyiofI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can find out the total number of horse and human images in the directories:"
      ],
      "metadata": {
        "id": "OO6AWc83iiDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'total training horse images: {len(os.listdir(train_horse_dir))}')\n",
        "print(f'total training human images: {len(os.listdir(train_human_dir))}')\n",
        "print(f'total validation horse images: {len(os.listdir(validation_horse_dir))}')\n",
        "print(f'total validation human images: {len(os.listdir(validation_human_dir))}')"
      ],
      "metadata": {
        "id": "NUwHyepHinB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now take a look at a few pictures to get a better sense of what they look like. First, configure the matplotlib parameters:"
      ],
      "metadata": {
        "id": "wQOonjHSiliu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "# Parameters for our graph; we'll output images in a 4x4 configuration\n",
        "nrows = 4\n",
        "ncols = 4\n",
        "\n",
        "# Index for iterating over images\n",
        "pic_index = 0"
      ],
      "metadata": {
        "id": "OINY2lelip44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, display a batch of 8 horse and 8 human pictures. You can rerun the cell to see a fresh batch each time:"
      ],
      "metadata": {
        "id": "nU315ewaiq_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up matplotlib fig, and size it to fit 4x4 pics\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(ncols * 4, nrows * 4)\n",
        "\n",
        "pic_index += 8\n",
        "next_horse_pix = [os.path.join(train_horse_dir, fname) \n",
        "                for fname in val_horse_names[pic_index-8:pic_index]]\n",
        "next_human_pix = [os.path.join(train_human_dir, fname) \n",
        "                for fname in val_human_names[pic_index-8:pic_index]]\n",
        "\n",
        "for i, img_path in enumerate(next_horse_pix+next_human_pix):\n",
        "  # Set up subplot; subplot indices start at 1\n",
        "  sp = plt.subplot(nrows, ncols, i + 1)\n",
        "  sp.axis('Off') # Don't show axes (or gridlines)\n",
        "\n",
        "  img = mpimg.imread(img_path)\n",
        "  plt.imshow(img)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l288ba7Eir42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a small model from scratch"
      ],
      "metadata": {
        "id": "kAW0C2Nxisms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 300x300 with 3 bytes color\n",
        "    # This is the first convolution\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # The second convolution\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The third convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fourth convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fifth convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans')\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "g0dSNOTgiv6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "nisrtxUUixEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=RMSprop(learning_rate=0.001),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "c4uQWbCQix9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# All images will be rescaled by 1./255\n",
        "train_datagen = ImageDataGenerator(rescale=1/255)\n",
        "validation_datagen = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "# Flow training images in batches of 128 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        './horse-or-human/',  # This is the source directory for training images\n",
        "        target_size=(300, 300),  # All images will be resized to 300x300\n",
        "        batch_size=128,\n",
        "        # Since you use binary_crossentropy loss, you need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "# Flow validation images in batches of 128 using validation_datagen generator\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        './validation-horse-or-human/',  # This is the source directory for validation images\n",
        "        target_size=(300, 300),  # All images will be resized to 300x300\n",
        "        batch_size=32,\n",
        "        # Since you use binary_crossentropy loss, you need binary labels\n",
        "        class_mode='binary')"
      ],
      "metadata": {
        "id": "TjG7UJ-iizLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "-jedSshQi1c0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=8,  \n",
        "      epochs=15,\n",
        "      verbose=1,\n",
        "      validation_data = validation_generator,\n",
        "      validation_steps=8)"
      ],
      "metadata": {
        "id": "Euk73toBi0qJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Prediction"
      ],
      "metadata": {
        "id": "6vUtbjZLi6nH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## CODE BLOCK FOR NON-SAFARI BROWSERS\n",
        "## SAFARI USERS: PLEASE SKIP THIS BLOCK AND RUN THE NEXT ONE INSTEAD\n",
        "\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        " \n",
        "  # predicting images\n",
        "  path = '/content/' + fn\n",
        "  img = image.load_img(path, target_size=(300, 300))\n",
        "  x = image.img_to_array(img)\n",
        "  x /= 255\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  images = np.vstack([x])\n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  print(classes[0])\n",
        "  if classes[0]>0.5:\n",
        "    print(fn + \" is a human\")\n",
        "  else:\n",
        "    print(fn + \" is a horse\")\n",
        " "
      ],
      "metadata": {
        "id": "VSSKRqG-i5bf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}